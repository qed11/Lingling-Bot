{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision\r\n",
    "import torch.utils.data as dt\r\n",
    "import torchvision.transforms as transforms\r\n",
    "import time\r\n",
    "import os\r\n",
    "import random\r\n",
    "from PIL import Image\r\n",
    "from dataset import AutoDataset\r\n",
    "from networks import AutoEncoder\r\n",
    "%matplotlib widget"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "batch = 64\r\n",
    "epoch = 30\r\n",
    "lr = 0.0001"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#function converts one folder of pictures into\r\n",
    "def autoencoder_dataset(path, set_percent = 0.1):\r\n",
    "    transform = transforms.Compose([transforms.Grayscale(), transforms.ToTensor(), transforms.Normalize((0.5), (0.5/3))])   #Make sure images are tensors, lie within 0 and 1\r\n",
    "    dataset = AutoDataset(path, transform = transform) #need to implement   #Turn the images into a dataset\r\n",
    "    set_size = int( set_percent * len( dataset ) )                                                        #Length of test and validation sets\r\n",
    "    return dt.random_split(dataset, [len( dataset ) - 2 * set_size, set_size, set_size])                #Return datasets in order of training set, validation set, test setde"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train, val, test = autoencoder_dataset('Data\\Hilbert\\Autoencoder')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_loader = dt.DataLoader(train, batch_size = 64)\r\n",
    "val_loader = dt.DataLoader(val, batch_size = 64)\r\n",
    "test_loader = dt.DataLoader(test, batch_size = 64)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def get_accuracy(model, loader, criterion, batch_size):\r\n",
    "    \"\"\" Compute the accuracy of the `model` across a dataset `data`\r\n",
    "    \r\n",
    "    Example usage:\r\n",
    "    \r\n",
    "    >>> model = MyRNN() # to be defined\r\n",
    "    >>> get_accuracy(model, valid) # the variable `valid` is from above\r\n",
    "    \"\"\"\r\n",
    "    model = model.cuda()\r\n",
    "    total_loss = 0.0\r\n",
    "    total_err = 0.0\r\n",
    "    total_epoch = 0\r\n",
    "    for i, picture in enumerate(loader, 0):\r\n",
    "        picture = picture.cuda()\r\n",
    "        outputs = model(picture)\r\n",
    "        loss = criterion(outputs, picture)\r\n",
    "        corr = torch.sum(torch.eq(picture, outputs))\r\n",
    "        #total_err += int(corr.sum())\r\n",
    "        total_loss += loss.item()\r\n",
    "        total_epoch += len(picture)\r\n",
    "    err = float(total_err) / total_epoch\r\n",
    "    loss = float(total_loss) / (i + 1) /batch_size\r\n",
    "    return err, loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def pathy(epoch, batch_size, lr):\r\n",
    "    model_path = os.path.join('Data','Training', f'epoch_{epoch}_batch{batch_size}_lr{lr}')\r\n",
    "    return model_path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def train_auto(net, batch_size = 64, lr = 0.01, epoch = 30):\r\n",
    "    net = net.cuda()\r\n",
    "    random.seed(1000)\r\n",
    "    np.random.seed(1000)\r\n",
    "    torch.manual_seed(1000)\r\n",
    "    torch.cuda.manual_seed(1000)\r\n",
    "    \r\n",
    "    criterion = nn.MSELoss()\r\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\r\n",
    "    \r\n",
    "    train_err = np.zeros(epoch)\r\n",
    "    train_loss = np.zeros(epoch)\r\n",
    "    val_err = np.zeros(epoch)\r\n",
    "    val_loss = np.zeros(epoch)\r\n",
    "    \r\n",
    "    start_time = time.time()\r\n",
    "    \r\n",
    "    for epoch in range(epoch):  # loop over the dataset multiple times\r\n",
    "        total_train_loss = 0.0\r\n",
    "        total_train_err = 0.0\r\n",
    "        total_epoch = 0\r\n",
    "        for i, picture in enumerate(train_loader):\r\n",
    "            \r\n",
    "            picture = picture.cuda()\r\n",
    "            #print(picture.shape)\r\n",
    "            \r\n",
    "            optimizer.zero_grad()\r\n",
    "            outputs = net(picture)\r\n",
    "            loss = criterion(outputs, picture)\r\n",
    "    \r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "\r\n",
    "            corr = torch.sum(torch.eq(picture, outputs))\r\n",
    "            total_train_err += int(corr.sum())\r\n",
    "            total_train_loss += loss.item()\r\n",
    "            total_epoch += len(picture)\r\n",
    "        \r\n",
    "        train_err[epoch] = float(total_train_err) / total_epoch\r\n",
    "        train_loss[epoch] = float(total_train_loss) /  (i+1) / batch_size\r\n",
    "        val_err[epoch], val_loss[epoch] = get_accuracy(net, val_loader, criterion, batch_size)\r\n",
    "        print((\"Epoch {}: Train loss: {} | \"+\r\n",
    "                \"Validation loss: {}\").format(\r\n",
    "                   epoch + 1,\r\n",
    "                   train_loss[epoch],\r\n",
    "                   val_loss[epoch]))\r\n",
    "        # Save the current model (checkpoint) to a file\r\n",
    "        model_path = pathy(epoch, batch_size, lr)\r\n",
    "        torch.save(net.state_dict(), model_path)\r\n",
    "    print('Finished Training')\r\n",
    "    end_time = time.time()\r\n",
    "    elapsed_time = end_time - start_time\r\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\r\n",
    "    # Write the train/test loss/err into CSV file for plotting later\r\n",
    "    #print(net.state_dict())\r\n",
    "    \r\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\r\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\r\n",
    "\r\n",
    "    return criterion"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def plot_training_curve(path):\r\n",
    "    \"\"\" Plots the training curve for a model run, given the csv files\r\n",
    "    containing the train/validation error/loss.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        path: The base path of the csv files produced during training\r\n",
    "    \"\"\"\r\n",
    "    import matplotlib.pyplot as plt\r\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\r\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\r\n",
    "    plt.title(\"Train vs Validation Error\")\r\n",
    "    n = len(train_loss) # number of epochs\r\n",
    "    plt.title(\"Train vs Validation Loss\")\r\n",
    "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\r\n",
    "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\r\n",
    "    plt.xlabel(\"Epoch\")\r\n",
    "    plt.ylabel(\"Loss\")\r\n",
    "    plt.legend(loc='best')\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "auto = AutoEncoder()\r\n",
    "crit = train_auto(auto, batch_size=batch, lr = lr, epoch = epoch)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss: 0.01392189982379878 | Validation loss: 0.008104333389214995\n",
      "Epoch 2: Train loss: 0.00706560218209209 | Validation loss: 0.0059569750060614254\n",
      "Epoch 3: Train loss: 0.005448610962582957 | Validation loss: 0.0051044769324715385\n",
      "Epoch 4: Train loss: 0.004805937300257248 | Validation loss: 0.004606304021543378\n",
      "Epoch 5: Train loss: 0.004449227181718126 | Validation loss: 0.004329169735144736\n",
      "Epoch 6: Train loss: 0.004209750569339649 | Validation loss: 0.004115877392750357\n",
      "Epoch 7: Train loss: 0.004003401618264726 | Validation loss: 0.003917428216857226\n",
      "Epoch 8: Train loss: 0.003825133114444853 | Validation loss: 0.0037520815424126755\n",
      "Epoch 9: Train loss: 0.003672015057779197 | Validation loss: 0.0036089447441718307\n",
      "Epoch 10: Train loss: 0.003547802323016744 | Validation loss: 0.0035051188687683565\n",
      "Epoch 11: Train loss: 0.003450681910568107 | Validation loss: 0.003413074020896619\n",
      "Epoch 12: Train loss: 0.003372480495013452 | Validation loss: 0.0033416268956445827\n",
      "Epoch 13: Train loss: 0.003306598168621056 | Validation loss: 0.003276312844033278\n",
      "Epoch 14: Train loss: 0.003251566762849245 | Validation loss: 0.003230553368653802\n",
      "Epoch 15: Train loss: 0.0032039303027851323 | Validation loss: 0.0031863306075228617\n",
      "Epoch 16: Train loss: 0.003164061346129558 | Validation loss: 0.003150879993584554\n",
      "Epoch 17: Train loss: 0.00313043834337968 | Validation loss: 0.0031197207904822455\n",
      "Epoch 18: Train loss: 0.0031017729634965356 | Validation loss: 0.0030974457603195125\n",
      "Epoch 19: Train loss: 0.0030765630844160514 | Validation loss: 0.0030689759177742835\n",
      "Epoch 20: Train loss: 0.0030530759649753463 | Validation loss: 0.003046523389004842\n",
      "Epoch 21: Train loss: 0.003032722876056506 | Validation loss: 0.003026543027319685\n",
      "Epoch 22: Train loss: 0.003013213167597519 | Validation loss: 0.00300595906598414\n",
      "Epoch 23: Train loss: 0.0029949346071930245 | Validation loss: 0.002988953193014055\n",
      "Epoch 24: Train loss: 0.002978065187013988 | Validation loss: 0.0029736251009725282\n",
      "Epoch 25: Train loss: 0.0029611785564813177 | Validation loss: 0.0029569237980709116\n",
      "Epoch 26: Train loss: 0.0029448955202090693 | Validation loss: 0.002941198044531577\n",
      "Epoch 27: Train loss: 0.0029301075109683427 | Validation loss: 0.0029253228280640324\n",
      "Epoch 28: Train loss: 0.002915534636134294 | Validation loss: 0.0029133347898139066\n",
      "Epoch 29: Train loss: 0.0029016660641632856 | Validation loss: 0.0028967221898702085\n",
      "Epoch 30: Train loss: 0.002888409560601682 | Validation loss: 0.0028852899327115\n",
      "Finished Training\n",
      "Total time elapsed: 2668.35 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "plot_training_curve(pathy(epoch=epoch - 1, batch_size=batch, lr = lr))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ee75a8b4cc34152838d694bf222be6c"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "torch.save(auto.state_dict(), os.path.join('Model', f'epoch{epoch}_batch{batch}_lr{lr}.pth'))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "4d1086b2def7ca966311c553e54c5365a2af11e0349a9b38cb8d60ef369f1c88"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}